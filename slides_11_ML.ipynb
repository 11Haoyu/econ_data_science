{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning \n",
    "\n",
    "Zhentao Shi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Machine learning and artificial intelligence:\n",
    "\n",
    "* Technology or alchemy?\n",
    "* Statistics or biology?\n",
    "\n",
    "* [Tom Sargent](https://www.project-syndicate.org/commentary/artificial-intelligence-new-economic-models-by-thomas-j-sargent-2019-11)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reference\n",
    "\n",
    "* [ISLR] James, Gareth., Witten, Daniela., Hastie, Trevor., & Tibshirani, Robert. (2017). An introduction to statistical learning.  (Open access at https://www.statlearning.com/)\n",
    "* [ESL] Friendman, Hastie and Tibshirani (2001, 2008): Elements of Statistical Learning (Open access at https://hastie.su.domains/Papers/ESLII.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Athey (2018) \n",
    "* Mullainathan and Spiess (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "* Connection between $X$ and $Y$\n",
    "* Regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A set of data fitting procedures focusing on out-of-sample prediction\n",
    "* Repeat a scientific experiment for $n$ times and obtain a dataset $(y_i, x_i)_{i=1}^n$.\n",
    "* How to best predict $y_{n+1}$ given $x_{n+1}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "* Only about $X$\n",
    "* Density estimation, principal component analysis, and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Conventional Statistics\n",
    "\n",
    "* Consistency\n",
    "* Asymptotic distribution (hopefully normal)\n",
    "* Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning's Responses\n",
    "\n",
    "* Efficiency is mostly irrelevant given big data\n",
    "* Statistical inference may not be the goal\n",
    "    * Recommendation system on Amazon or Taobao\n",
    "    * Care about the prediction accuracy, not the causal link\n",
    "* Is there a data generating process (DGP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonparametric Estimation\n",
    "\n",
    "* *Parametric*: a finite number of parameters\n",
    "* *Nonparametric*: an infinite number of parameters\n",
    "\n",
    "* Some ideas in nonparametric estimation is directly related to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Density Estimation\n",
    "\n",
    "* Density estimation given a sample $(x_1,\\ldots,x_n)$\n",
    "* If drawn from a parametric family, MLE for estimation\n",
    "* Misspecification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Histogram is nonparametric\n",
    "    * If grid too fine, small bias but large variance\n",
    "    * If grid too coarse, small variance but large bias\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAADFBMVEUAAADT09P/AAD////u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\ngAAQCQABIBIAAvwDd2iWC1g5I9cAAAAASUVORK5CYII=",
      "text/plain": [
       "Plot with title \"\""
      ]
     },
     "metadata": {
      "image/png": {
       "height": 420,
       "width": 420
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n <- 200\n",
    "\n",
    "par(mfrow = c(3, 3))\n",
    "par(mar = c(1, 1, 1, 1))\n",
    "\n",
    "x_base <- seq(0.01,1,by = 0.01)\n",
    "breaks_list = c(4, 12, 60)\n",
    "\n",
    "for (ii in 1:3){\n",
    "  x <- rbeta(n, 2, 2) # beta distribution\n",
    "  for ( bb in breaks_list){\n",
    "    hist(x, breaks = bb, main=\"\", freq = FALSE, ylim = c(0,3),xlim = c(0,1))\n",
    "    lines( y = dbeta( x_base, 2, 2), x = x_base , col = \"red\" )\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variance-Bias Tradeoff\n",
    "\n",
    "![](graph/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Conditional Mean\n",
    "\n",
    "* Conditional mean $$f(x) = E[y_i |x_i = x]$$ given a sample $(y_i, x_i)$. \n",
    "* Solve \n",
    "$$\n",
    "\\min_f E[ (y_i - f(x_i) )^2 ]\n",
    "$$\n",
    "* In general $f(x)$ is a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Restrict the class of functions to search for minimizer\n",
    "    * Assume differentiability\n",
    "* One way is kernel method based on density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Series Estimation\n",
    "\n",
    "* Series expansion to approximate $f(x)$\n",
    "* Generates many additive regressors\n",
    "    * Ex: bounded, continuous and differentiate function has a series\n",
    "representation $f(x) = \\sum_{k=0}^{\\infty} \\beta_k \\cos (\\frac{k}{2}\\pi x )$.\n",
    "    * In finite sample, choose a finite $K$, usually much smaller than $n$\n",
    "    * Asymptotically $K \\to \\infty$ as $n \\to \\infty$ so that\n",
    "$$\n",
    "f_K(x) = \\sum_{k=0}^{K} \\beta_k \\cos \\left(\\frac{k}{2}\\pi x \\right) \\to f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Bias-variance trade-off\n",
    "    * Big $K$: small bias and large variance \n",
    "    * Small $K$: small variance and large bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Penalization\n",
    "\n",
    "* Specify a sufficiently large $K$, and then add a penalty term to control the complexity\n",
    "* Eg: *Ridge regression*: \n",
    "$$\n",
    "\\min_\\beta \\  \\frac{1}{2n}  \\sum_{i=1}^n \\left(y_i - \\sum_{k=0}^{K} \\beta_k f_k(x_i) \\right)^2\n",
    "+ \\lambda \\sum_{k=0}^K \\beta_k^2,\n",
    "$$\n",
    "where $\\lambda$ is the tuning parameter such that $\\lambda \\to 0$ as $n\\to \\infty$, and\n",
    "$f_k(x_i) = \\cos \\left(\\frac{k}{2}\\pi x_i \\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In compact notation, let $Y=(y_1,\\ldots,y_n)'$ and\n",
    "$X = (X_{ik} = f_k(x_i) )$, the above problem can be written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_2 ^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning Parameter\n",
    "\n",
    "* *Information criterion*: AIC, BIC\n",
    "* *Cross validation*\n",
    "\n",
    "\n",
    "* Active statistical research, but has little economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Econometrics Workflow\n",
    "\n",
    "![](graph/metric_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "![ ](graph/ML_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Data Splitting\n",
    "\n",
    "\n",
    "<!-- \\begin{figure} -->\n",
    "<!-- \\centering -->\n",
    "<!-- \\begin{tikzpicture}[node distance = 10 mm, thick, scale=1, transform shape] -->\n",
    "<!-- % Nodes -->\n",
    "<!--   \\node[ellipse, draw] -->\n",
    "<!--     (a) [label=above left:$data$]{Training Data}; -->\n",
    "<!--   \\node[ellipse, draw, right = of a] -->\n",
    "<!--     (b) {Validation Data}; -->\n",
    "<!--   \\node[ellipse, draw=red,fill=black!0, right = of b, text width=3cm, align=center] -->\n",
    "<!--     (c) {(Out of Sample) Testing Data}; -->\n",
    "<!--   \\node[ellipse, draw=blue,fill=black!0, below right = 20 mm of a] -->\n",
    "<!--     (d) {Fitted model} ; -->\n",
    "<!--   \\node[ellipse, draw=blue,fill=black!0, right= 10 mm of d, text width=3.5cm, align=center] -->\n",
    "<!--     (e) {Best tuning Parameter (Model)}; -->\n",
    "<!--   \\node[ellipse, draw, left = 20 mm of d, loosely dashed] -->\n",
    "<!--     (f) -->\n",
    "<!--     [label = below: \\textcolor{black!40}{Many Sets of Tuning Parameters}] -->\n",
    "<!--     {Model}; -->\n",
    "\n",
    "<!-- % Arrows -->\n",
    "<!--     \\draw [->, black] (a) -- (d); -->\n",
    "<!--     \\draw [->, blue] (d) -- (b); -->\n",
    "<!--     \\draw [->, blue] (b) -- (e); -->\n",
    "<!--     \\draw [->, blue] (e) -- (c); -->\n",
    "<!--     \\draw [->, black, loosely dashed] (f) -- (d); -->\n",
    "\n",
    "<!-- % Caption     -->\n",
    "<!--   \\node[below = of d] { -->\n",
    "<!--         \\begin{tabular}{l} -->\n",
    "<!--             $\\bullet$ Data splitting can be done by cross validation \\\\ -->\n",
    "<!--             $\\bullet$ A data driven approach for feature selection -->\n",
    "<!--         \\end{tabular}}; -->\n",
    "<!-- \\end{tikzpicture} -->\n",
    "<!-- \\caption{Learning workflow} -->\n",
    "<!-- \\end{figure} -->\n",
    "\n",
    "\n",
    "The workflow of machine learning methods is quite different from econometrics. The main purpose is often prediction instead of interpretation.\n",
    "They use some \"off-the-shelf\" generic learning methods, and\n",
    "the models are measured by their performance in prediction.\n",
    "In order to avoid overfitting it is essential to tune at least a few tuning parameters.\n",
    "\n",
    "Most machine learning methods take an agnostic view about the DGP, and they explicitly\n",
    "acknowledge model uncertainty. To address the issue of model selection (tuning parameter selection),\n",
    "in a data rich environment we split the data into three parts. A *training dataset*\n",
    "is used to fit the model parameter given the tuning parameters. A *validation dataset* is\n",
    "used to compare the out-of-sample performance under different tuning parameters.\n",
    "It helps decide a set of desirable tuning parameters. Ideally, the *testing sample* should be\n",
    "kept by a third party away from the modeler. The testing sample is the final\n",
    "judge of the relative merit of the fitted models.\n",
    "\n",
    "The R package `caret` (Classification And REgression Training) provides a framework for many machine learning methods.\n",
    "The function [`createDataPartition`](https://topepo.github.io/caret/data-splitting.html)\n",
    "splits the sample for both cross-sectional data and time series.\n",
    "\n",
    "\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "An $S$-fold cross validation partitions the dataset into $S$ disjoint sections. In each iteration, it picks one of the sections as the validation sample and the other $S-1$ sections as the training sample, and computes an out-of-sample goodness-of-fit measurement, for example *mean-squared prediction error* ${n_v}^{-1} \\sum_{i \\in val} (y_i - \\hat{y}_i)^2$ where $val$ is the validation set and $n_v$ is its cardinality,  or *mean-absolute prediction error* ${n_v}^{-1}\\sum_{i \\in val} |y_i - \\hat{y}_i|$. Repeat this process for $S$ times so that each of the $S$ sections are treated as the validation sample, and average the goodness-of-fit measurement over the $S$ sections to determined the best tuning parameter. If $S=n-1$, it is called *leave-one-out cross validation*, but it can be computationally too expensive when  $n$ is big. Instead, in practice we can  $S=5$ for 10, called 5-fold cross validation or 10-fold cross validation, respectively.\n",
    "\n",
    "\n",
    "\n",
    "\\begin{figure}\n",
    "\\centering\n",
    "\\includegraphics[width = 14cm]{graph/CV_Figure}\n",
    "\\caption{Rolling window time series cross validation}\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "\n",
    "In time series context, cross validation must preserve the dependence structure. If the time series is stationary, we can partition the data into $S$ consecutive blocks. If the purpose is ahead-of-time forecasting, then we can use nested CV. The figure shows a nested CV with fixed-length rolling window scheme, while the sub-training data can also be an extending rolling window. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Variable Selection and Prediction\n",
    "\n",
    "In modern scientific analysis, the number of covariates $x_i$ can be enormous.\n",
    "In DNA microarray analysis, we look for association between a symptom and genes.\n",
    "Theory in biology indicates that only a small handful of genes are involved,\n",
    "but it does not pinpoint which ones are the culprits.\n",
    "Variable selection is useful to identify the relevant genes, and then we can\n",
    "think about how to edit the genes to prevent certain diseases and better people's life.\n",
    "\n",
    "Explanatory variables are abundant in some empirical economic examples.\n",
    "For instance, a questionnaire from the [UK Living Costs and Food Survey](https://discover.ukdataservice.ac.uk/series/?sn=2000028), a survey\n",
    "widely used for analysis of demand theory and family consumption,\n",
    "consists of thousand of questions.\n",
    "@giannone2017economic [link](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3031893) experiment variable selection methods in 6 widely used economic datasets with many predictors.\n",
    "\n",
    "**Hazard of model selection** To elaborate the distortion of test size when the $t$ statistic is selected from two models in pursuit of significance.\n",
    "$$\n",
    "\\begin{pmatrix}y\\\\\n",
    "x_{1}\\\\\n",
    "x_{2}\n",
    "\\end{pmatrix}\\sim N\\left(0,\\begin{pmatrix}1 & 0 & 0\\\\\n",
    "0 & 1 & \\sqrt{0.5}\\\\\n",
    "0 & \\sqrt{0.5} & 1\n",
    "\\end{pmatrix}\\right)\n",
    "$$\n",
    "Both $x_1$ and $x_2$ are independent of $y$. The test size dependens on the correlation between the two regressors.\n",
    "If the test is conducted for a single model, the size is the pre-specified 10%.\n",
    "If we try two models, the size is inflated to about 17%. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- 100\n",
    "Rep <- 5000\n",
    "\n",
    "t_stat <- function(y, x) {\n",
    "  beta_hat <- sum(x * y) / sum(x^2)\n",
    "  e_hat <- y - beta_hat * x\n",
    "  sigma2_hat <- var(e_hat)\n",
    "  t_stat <- beta_hat / sqrt(sigma2_hat / sum(x^2))\n",
    "  return(t_stat)\n",
    "}\n",
    "\n",
    "res <- matrix(NA, Rep, 2)\n",
    "\n",
    "for (r in 1:Rep) {\n",
    "  y <- rnorm(n)\n",
    "  x1 <- rnorm(n)\n",
    "  x2 <- sqrt(0.5) * x1 + sqrt(0.5) * rnorm(n)\n",
    "\n",
    "  res[r, ] <- c(t_stat(y, x1), t_stat(y, x2))\n",
    "}\n",
    "\n",
    "print(mean(apply(abs(res), 1, max) > qnorm(0.95)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Caret` Package\n",
    "\n",
    "* R package `caret` (Classification And REgression Training): a framework for many machine learning methods\n",
    "* The function [`createDataPartition`](https://topepo.github.io/caret/data-splitting.html)\n",
    "splits the sample for both cross sectional data and time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection\n",
    "\n",
    "* Number of covariates $x_i$ can be enormous.\n",
    "    * DNA microarray\n",
    "    * UK Living Costs and Food Survey\n",
    "    * Giannone, Lenza, and Primiceri (2017): Illusion of sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Conventional attitude: prior knowledge\n",
    "* Recently economists wake up from the long lasting negligence.\n",
    "    * Stock and Watson (2012): forecasting 143 US macroeconomic indicators.\n",
    "    * A horse race of several variable selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lasso\n",
    "\n",
    "* least-absolute-shrinkage-and-selection-operator\n",
    "(Lasso) (Tibshirani, 1996)\n",
    "* Penalizes the $L_1$ norm of the coefficients.\n",
    "The criterion function of Lasso is written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_1\n",
    "$$\n",
    "where $\\lambda \\geq 0$ is a tuning parameter. \n",
    "\n",
    "Lasso shrinks some coefficients exactly to 0, in a wide range of values of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![ ](graph/lasso_regression2.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "n <- 40\n",
    "p <- 50\n",
    "b0 <- c(rep(1, 10), rep(0, p - 10))\n",
    "x <- matrix(rnorm(n * p), n, p)\n",
    "y <- x %*% b0 + rnorm(n)\n",
    "\n",
    "ols <- MASS::ginv(t(x) %*% x) %*% (t(x) %*% y) # OLS\n",
    "# Implement Lasso by glmnet\n",
    "cv_lasso <- glmnet::cv.glmnet(x, y)\n",
    "lasso_result <- glmnet::glmnet(x, y, lambda = cv_lasso$lambda.min)\n",
    "\n",
    "# Get weights\n",
    "b_temp <- as.numeric(lasso_result$beta)\n",
    "b_temp[b_temp == 0] <- 1e-8\n",
    "w <- 1 / abs(b_temp) # Let gamma = 1\n",
    "\n",
    "# Implement Adaptive Lasso by glmnet\n",
    "cv_alasso <- glmnet::cv.glmnet(x, y, penalty.factor = w)\n",
    "alasso_result <-\n",
    "  glmnet::glmnet(x, y, penalty.factor = w, lambda = cv_alasso$lambda.min)\n",
    "\n",
    "plot(b0, ylim = c(-0.8, 1.5), pch = 4, xlab = \"\", ylab = \"coefficient\")\n",
    "points(lasso_result$beta, col = \"red\", pch = 6)\n",
    "points(alasso_result$beta, col = \"blue\", pch = 5)\n",
    "points(ols, col = \"green\", pch = 3)\n",
    "\n",
    "# out of sample prediction\n",
    "x_new <- matrix(rnorm(n * p), n, p)\n",
    "y_new <- x_new %*% b0 + rnorm(n)\n",
    "lasso_msfe <- (y_new - predict(lasso_result, newx = x_new)) %>% var()\n",
    "alasso_msfe <- (y_new - predict(alasso_result, newx = x_new)) %>% var()\n",
    "ols_msfe <- (y_new - x_new %*% ols) %>% var()\n",
    "\n",
    "print(c(lasso_msfe, alasso_msfe, ols_msfe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can DIY Lasso by `CVXR`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "library(CVXR)\n",
    "\n",
    "lambda <- 2 * cv_lasso$lambda.min # tuning parameter\n",
    "\n",
    "# CVXR for Lasso\n",
    "beta_cvxr <- Variable(p)\n",
    "obj <- sum_squares(y - x %*% beta_cvxr) / (2 * n) + lambda * p_norm(beta_cvxr, 1)\n",
    "prob <- Problem(Minimize(obj))\n",
    "lasso_cvxr <- solve(prob)\n",
    "beta_cvxr_hat <- lasso_cvxr$getValue(beta_cvxr) %>% as.vector() %>% print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SCAD\n",
    "\n",
    "* Smoothly-clipped-absolute-deviation (SCAD) Fan and Li (2001):\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\sum_{j=1}^d \\rho_{\\lambda}( |\\beta_j| )\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\rho_{\\lambda}^{\\prime} (\\theta) = \\lambda \\left\\{ 1\\{\\theta\\leq \\lambda \\} +\n",
    "\\frac{(a\\lambda - \\theta)_+}{(a-1)\\lambda} \\cdot 1 \\{\\theta > \\lambda\\} \\right\\}\n",
    "$$\n",
    "for some $a>2$ and $\\theta>0$. \n",
    "\n",
    "* SCAD enjoys *oracle property*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](SCAD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaptive Lasso\n",
    "\n",
    "*Adaptive Lasso* (Zou, 2006) also enjoys the oracle property.\n",
    "\n",
    "Two-step algorithm:\n",
    "1. First run a Lasso or ridge regression and save the estimator $\\hat{\\beta}^{(1)}$\n",
    "2. Solve \n",
    "\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\sum_{j=1}^d  w_j |\\beta_j|\n",
    "$$ \n",
    "\n",
    "where $w_j = 1 /  |\\hat{\\beta}_j^{(1)} |^a$ and $a\\geq 1$ is a constant. (Common choice is $a = 1$ or 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "R packages\n",
    "\n",
    "* `glmnet` or `LARS` implements Lasso\n",
    "* `ncvreg` carries out SCAD. \n",
    "* Adaptive Lasso by setting the weight via the argument `penalty.factor` in `glmnet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stagewise Forward Selection\n",
    "\n",
    "More methods are available if prediction of the response variables is the sole purpose of the regression.\n",
    "\n",
    "Eg: *stagewise forward selection*\n",
    "\n",
    "1. Start from an empty model. \n",
    "2. Given many candidate $x_j$, in each round we add the regressor that can\n",
    "produce the biggest $R^2$. \n",
    "\n",
    "Close to the idea of *$L_2$ componentwise boosting*\n",
    "which does not adjust the coefficients fitted earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Second Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction-Oriented Methods\n",
    "\n",
    "* Methods that induces data-driven interaction of the covariates.\n",
    "* Interaction makes the covariates much more flexible\n",
    "* Insufficient theoretical understanding\n",
    "* \"Black-boxes\" methods\n",
    "\n",
    "* Surprisingly superior performance\n",
    "* Industry insiders are pondering \"alchemy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Tree\n",
    "\n",
    "* Supervised learning: $x \\to y $\n",
    "* Traditional nonparametric methods: kernel or series\n",
    "\n",
    "* Regression tree (Breiman, 1984) recursively partitions the space of the regressors\n",
    "    * Each time a covariate is split into two dummies\n",
    "    * Splitting criterion is aggressive reduction of the SSR\n",
    "    \n",
    "    * Tuning parameter is the depth of the tree\n",
    "    * Given a dataset $d$ and the depth of the tree, the fitted tree $\\hat{r}(d)$ is deterministic\n",
    "\n",
    "\n",
    "- Example: Using longitude and latitude for Beijing housing price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging\n",
    "\n",
    "* Tree is unstable\n",
    "* *Bootstrap averaging*, or *bagging*, reduces variance of trees (Breiman, 1996)\n",
    "    * Grow a tree for each bootstrap sample\n",
    "    * Simple average\n",
    "\n",
    "* An example of the *ensemble learning*.\n",
    "\n",
    "* Inoue and Kilian (2008): an early application of bagging in time series forecast.\n",
    "* Hirano and Wright (2017): a theoretical perspective on the risk reduction of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "* *Random forest* (Breiman, 2001):\n",
    "    * Draw a bootstrap sample\n",
    "    * Before each split, shakes up the regressors by randomly sampling $m$ out of the total $p$ covarites. Stop until the depth of the tree is reached.\n",
    "    * Average the trees over the bootstrap samples\n",
    "    \n",
    "* The tuning parameters are the tree depth and $m$\n",
    "* More stable than bagging thanks to \"de-correlation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: randomForest\n",
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "Loading required package: MASS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>IncNodePurity</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>crim</th><td>1476.8398</td></tr>\n",
       "\t<tr><th scope=row>zn</th><td> 138.0405</td></tr>\n",
       "\t<tr><th scope=row>indus</th><td> 930.0199</td></tr>\n",
       "\t<tr><th scope=row>chas</th><td> 265.7788</td></tr>\n",
       "\t<tr><th scope=row>nox</th><td>1831.5730</td></tr>\n",
       "\t<tr><th scope=row>rm</th><td>7551.2292</td></tr>\n",
       "\t<tr><th scope=row>age</th><td> 567.3393</td></tr>\n",
       "\t<tr><th scope=row>dis</th><td>1296.5395</td></tr>\n",
       "\t<tr><th scope=row>rad</th><td> 280.9853</td></tr>\n",
       "\t<tr><th scope=row>tax</th><td> 639.1153</td></tr>\n",
       "\t<tr><th scope=row>ptratio</th><td>1990.4504</td></tr>\n",
       "\t<tr><th scope=row>black</th><td> 469.8464</td></tr>\n",
       "\t<tr><th scope=row>lstat</th><td>7076.4031</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & IncNodePurity\\\\\n",
       "\\hline\n",
       "\tcrim & 1476.8398\\\\\n",
       "\tzn &  138.0405\\\\\n",
       "\tindus &  930.0199\\\\\n",
       "\tchas &  265.7788\\\\\n",
       "\tnox & 1831.5730\\\\\n",
       "\trm & 7551.2292\\\\\n",
       "\tage &  567.3393\\\\\n",
       "\tdis & 1296.5395\\\\\n",
       "\trad &  280.9853\\\\\n",
       "\ttax &  639.1153\\\\\n",
       "\tptratio & 1990.4504\\\\\n",
       "\tblack &  469.8464\\\\\n",
       "\tlstat & 7076.4031\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | IncNodePurity |\n",
       "|---|---|\n",
       "| crim | 1476.8398 |\n",
       "| zn |  138.0405 |\n",
       "| indus |  930.0199 |\n",
       "| chas |  265.7788 |\n",
       "| nox | 1831.5730 |\n",
       "| rm | 7551.2292 |\n",
       "| age |  567.3393 |\n",
       "| dis | 1296.5395 |\n",
       "| rad |  280.9853 |\n",
       "| tax |  639.1153 |\n",
       "| ptratio | 1990.4504 |\n",
       "| black |  469.8464 |\n",
       "| lstat | 7076.4031 |\n",
       "\n"
      ],
      "text/plain": [
       "        IncNodePurity\n",
       "crim    1476.8398    \n",
       "zn       138.0405    \n",
       "indus    930.0199    \n",
       "chas     265.7788    \n",
       "nox     1831.5730    \n",
       "rm      7551.2292    \n",
       "age      567.3393    \n",
       "dis     1296.5395    \n",
       "rad      280.9853    \n",
       "tax      639.1153    \n",
       "ptratio 1990.4504    \n",
       "black    469.8464    \n",
       "lstat   7076.4031    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Plot with title \"Boston.rf\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "require(randomForest)\n",
    "require(MASS)#Package which contains the Boston housing dataset\n",
    "attach(Boston)\n",
    "set.seed(101)\n",
    "\n",
    "#training Sample with 300 observations\n",
    "train=sample(1:nrow(Boston),300)\n",
    "\n",
    "Boston.rf=randomForest(medv ~ . , data = Boston, subset = train)\n",
    "plot(Boston.rf)\n",
    "\n",
    "# getTree(Boston.rf)\n",
    "importance(Boston.rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Consistency of random forest is not proved\n",
    "until Scornet, Biau, and Vert (2015)\n",
    "* Inferential theory was first established by\n",
    "Wager Athey (2018)  in the context of treatment effect estimation\n",
    "* Athey, Tibshirani, and Wager (2019) generalizes CART to local maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "* Bagging and random forest use equal weight on each generated tree for the ensemble\n",
    "* Tree boosting takes a deterministic approach for the weights\n",
    "    1. Use the original data $d^0=(x_i,y_i)$ to grow a shallow tree $\\hat{r}^{0}(d^0)$. Save the prediction $f^0_i = \\alpha \\cdot \\hat{r}^0 (d^0, x_i)$ where\n",
    "   $\\alpha\\in [0,1]$ is a shrinkage tuning parameter. Save\n",
    "   the residual $e_i^{0} = y_i - f^0_i$. Set $m=1$.\n",
    "    2. In the $m$-th iteration, use the data $d^m = (x_i,e_i^{m-1})$ to grow a shallow tree $\\hat{r}^{m}(d^m)$. Save the prediction $f^m_i =  f^{m-1}_i +  \\alpha \\cdot \\hat{r}^m (d, x_i)$. Save\n",
    "   the residual $e_i^{m} = y_i - f^m_i$. Update $m = m+1$.\n",
    "    3. Repeat Step 2 until $m > M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Boosting has three tuning parameters: the tree depth,  the shrinkage level $\\alpha$, and the number of iterations $M$\n",
    "* The algorithm can be sensitive to any of the three tuning parameters\n",
    "* When a model is tuned well, it can performs remarkably\n",
    "    * Example: Beijing housing data.\n",
    "    * Gradient boosting via the package `gbm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Statisticians view boosting as a gradient descent algorithm to reduce the risk. The fitted\n",
    "tree in each iteration is the deepest descent direction, while the shrinkage tames the fitting to avoid proceeding too aggressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Attaching package: 'ggplot2'\n",
      "\n",
      "The following object is masked from 'package:randomForest':\n",
      "\n",
      "    margin\n",
      "\n",
      "Loading required package: foreach\n",
      "Loading required package: iterators\n",
      "Loading required package: parallel\n",
      "Warning message in readChar(con, 5L, useBytes = TRUE):\n",
      "\"cannot open compressed file 'lianjia.RData', probable reason 'No such file or directory'\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection\nTraceback:\n",
      "1. load(\"lianjia.RData\")",
      "2. readChar(con, 5L, useBytes = TRUE)"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(doParallel)\n",
    "\n",
    "load(\"lianjia.RData\")\n",
    "\n",
    "price_reg=price~\n",
    "  t_trade+age+fiveYearsProperty+subway+factor(district)+dist_center+\n",
    "  square+livingRoom+drawingRoom+kitchen+bathRoom+\n",
    "  factor(floor_type)+floor_total+elevator+ladderRatio+\n",
    "  factor(renovationCondition)+factor(buildingType)+factor(buildingStructure)+\n",
    "  communityAverage+DOM+followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# GBM\n",
    "## Tuning Parameters\n",
    "\n",
    "gbmGrid=expand.grid(interaction.depth=10:14,\n",
    "                    n.trees=(15:20)*100,\n",
    "                    shrinkage=c(0.01,0.05,0.1),\n",
    "                    n.minobsinnode=20)\n",
    "gbmControl=trainControl(method=\"repeatedcv\",number=5,repeats=1)\n",
    "\n",
    "registerDoParallel(8)\n",
    "t=Sys.time()\n",
    "boostingReg=train(price_reg,data=lianjia[train_tune,],\n",
    "                  method=\"gbm\",distribution=\"gaussian\",\n",
    "                  trControl=gbmControl,tuneGrid=gbmGrid,metric=\"Rsquared\",\n",
    "                  verbose=F)\n",
    "cat(\"Time Cost of Finding Best Tuning Parameters:\",Sys.time()-t,\"\\n\")\n",
    "stopImplicitCluster()\n",
    "\n",
    "gbmTune=boostingReg$bestTune\n",
    "cat(\"The best tuning parameters for GBM are: \\n\");print(gbmTune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Estimation and Prediction\n",
    "\n",
    "train_ind=createDataPartition(1:nrow(lianjia),p=0.75)$Resample1\n",
    "boostingReg=train(price_reg,data=lianjia[train_ind,],method=\"gbm\",\n",
    "                  distribution=\"gaussian\", # to decide the lost function\n",
    "                  tuneGrid=gbmTune,verbose=F)\n",
    "pred.boosting=predict(boostingReg,newdata=lianjia[-train_ind,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# LM\n",
    "\n",
    "lmReg=lm(price_reg,data=lianjia[train_ind,])\n",
    "pred.lm=predict(lmReg,newdata=lianjia[-train_ind,])\n",
    "\n",
    "\n",
    "# Comparison\n",
    "\n",
    "target=lianjia[-train_ind,]$price\n",
    "cat(\"R-squared of GBM prediction =\",miscTools::rSquared(target,target-pred.boosting),\"\\n\")\n",
    "cat(\"R-squared of LM prediction =\",miscTools::rSquared(target,target-pred.lm),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Many variants of boosting algorithms\n",
    "    * $L_2$-boosting\n",
    "    * componentwise boosting\n",
    "    * AdaBoosting, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "* Artificial neural network (ANN) is the workhorse behind Alpha-Go and self-driven cars\n",
    "* A particular type of nonlinear models.\n",
    "\n",
    "![ANN](Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "z_l^{(k)} & = & w_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = & g^{(k)} ( z_l^{(k)}), \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "where $ a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $g(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choice: sigmoid ($1/(1+\\exp(-x))$); ReLu, $z\\cdot 1\\{x\\geq 0\\}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A user has several decisions to make\n",
    "* Activation function\n",
    "* Number of hidden layers\n",
    "* Number of nodes in each layer\n",
    "\n",
    "\n",
    "* Many free parameters are generated from the multiple layer and multiple nodes\n",
    "* In estimation often regularization methods are employed to penalize\n",
    "the $l_1$ and/or $l_2$ norms, which requires extra tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Theory is Underdeveloped\n",
    "\n",
    "* Theoretical understanding about its behavior is scant\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "    * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any\n",
    "measurable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computation\n",
    "\n",
    "* Free parameters must be determined by\n",
    "numerical optimization\n",
    "* Nonlinear complex structure makes the optimization\n",
    "very challenging and the global optimizer is beyond guarantee\n",
    "* De facto optimization algorithm\n",
    "is *stochastic gradient descent*\n",
    "\n",
    "* Google's `tensorflow`\n",
    "* `keras` is the deep learning modeling language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In optimization we update the parameter\n",
    "$$\n",
    "\\beta_{k+1} = \\beta_{k} + a_k p_k,\n",
    "$$\n",
    "where $a_k \\in \\mathbb{R}$ is the step length and $p_k$ is a vector\n",
    "of directions. \n",
    "\n",
    "Use a Talyor expansion,\n",
    "$$\n",
    "f(\\beta_{k+1}) = f(\\beta_k + a_k p_k ) \\approx f(\\beta_k) + a_k \\nabla f(\\beta_k) p_k,\n",
    "$$\n",
    "\n",
    "\n",
    "If in each step we want the value of the criterion function\n",
    "$f(x)$ to decrease, we need $\\nabla f(\\beta_k) p_k \\leq 0$.\n",
    "A simple choice is $p_k =-\\nabla f(\\beta_k)$.\n",
    "\n",
    "Newton's method corresponds to $p_k =- (\\nabla^2 f(\\beta_k))^{-1}  \\nabla f(\\beta_k)$,\n",
    "and BFGS uses a low-rank matrix to approximate $\\nabla^2 f(\\beta_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* When sample size and/or number of parameter is big, evaluation of the gradient can be prohibitively expensive.\n",
    "* SGD uses a small batch of the sample to evaluate the gradient in each iteration. \n",
    "\n",
    "* SGD involves tuning parameters (say, the batch size and the learning rate) that can dramatically affect\n",
    "the outcome, in particular in nonlinear problems.\n",
    "* Careful experiments must be carried out before serious implementation.\n",
    "\n",
    "Below is an example of SGD in the PPMLE with sample size 100,000 and\n",
    "the number of parameters 100. SGD is usually much faster.\n",
    "\n",
    "The new functions are defined with the data explicity as arguments.\n",
    "Because in SGD each time the log-likelihood function and the gradiant are\n",
    "evaluated at a different subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "poisson.loglik = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  exp( X %*% b )\n",
    "  ell = -mean( -lambda + y *  log(lambda) )\n",
    "  return(ell)\n",
    "}\n",
    "\n",
    "\n",
    "poisson.loglik.grad = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  as.vector( exp( X %*% b ) )\n",
    "  ell = -colMeans( -lambda * X + y * X )\n",
    "  ell_eta = ell\n",
    "  return(ell_eta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "##### generate the artificial data\n",
    "set.seed(898)\n",
    "nn = 1e5; K = 100\n",
    "\n",
    "X = cbind(1, matrix( runif( nn*(K-1) ), ncol = K-1 ) )\n",
    "b0 = rep(1, K) / K\n",
    "y = rpois(nn, exp( X %*% b0 ) )\n",
    "\n",
    "\n",
    "b.init = runif(K); b.init  = 2 * b.init / sum(b.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# and these tuning parameters are related to N and K\n",
    "\n",
    "n = length(y)\n",
    "test_ind = sample(1:n, round(0.2*n) )\n",
    "\n",
    "y_test = y[test_ind]\n",
    "X_test = X[test_ind, ]\n",
    "\n",
    "y_train = y[-test_ind ]\n",
    "X_train = X[-test_ind, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "\n",
    "# sgd depends on\n",
    "# * eta: the learning rate\n",
    "# * epoch: the averaging small batch\n",
    "# * the initial value\n",
    "\n",
    "set.seed(105)\n",
    "\n",
    "max_iter = 5000\n",
    "min_iter = 20\n",
    "eta=0.01\n",
    "epoch = round( 100*sqrt(K) )\n",
    "\n",
    "\n",
    "b_old = b.init\n",
    "\n",
    "pts0 = Sys.time()\n",
    "# the iteration of gradient\n",
    "for (i in 1:max_iter ){\n",
    "\n",
    "  loglik_old = poisson.loglik(b_old, y_train, X_train)\n",
    "  i_sample = sample(1:length(y_train), epoch, replace = TRUE )\n",
    "  b_new = b_old - eta * poisson.loglik.grad(b_old, y_train[i_sample], X_train[i_sample, ])\n",
    "  loglik_new = poisson.loglik(b_new, y_test, X_test)\n",
    "  b_old = b_new # update\n",
    "\n",
    "  criterion =  loglik_old - loglik_new  \n",
    "  if (  criterion < 0.0001 & i >= min_iter ) break\n",
    "}\n",
    "cat(\"point estimate =\", b_new, \", log_lik = \", loglik_new, \"\\n\")\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# optimx is too slow for this dataset.\n",
    "# Nelder-Mead method is too slow for this dataset\n",
    "\n",
    "# thus we only sgd with NLoptr\n",
    "\n",
    "opts = list(\"algorithm\"=\"NLOPT_LD_SLSQP\",\"xtol_rel\"=1.0e-7, maxeval = 5000)\n",
    "\n",
    "\n",
    "pts0 = Sys.time( )\n",
    "res_BFGS = nloptr::nloptr( x0=b.init,\n",
    "                 eval_f=poisson.loglik,\n",
    "                 eval_grad_f = poisson.loglik.grad,\n",
    "                 opts=opts,\n",
    "                 y = y_train, X = X_train)\n",
    "print( res_BFGS )\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)\n",
    "\n",
    "b_hat_nlopt = res_BFGS$solution\n",
    "\n",
    "\n",
    "#### evaluation in the test sample\n",
    "cat(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
    "cat(\"log lik in test data by sgd = \", poisson.loglik(b_new, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by nlopt = \", poisson.loglik(b_hat_nlopt, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by oracle = \", poisson.loglik(b0, y = y_test, X_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* Mature algorithms for implementation\n",
    "* Theoretical investigation is in progress\n",
    "* Economic applications are just beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Lehrer and Xie (2017) \n",
    "* Feng, Giglio, and Xiu (2019)\n",
    "* Chinco, Clark-Joseph, and Ye (2019)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": "",
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "rise": {
   "enable_chalkboard": true,
   "scroll": true,
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
