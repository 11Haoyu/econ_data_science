{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Lianjia Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We applied machine learning methods by training on a Beijing Lianjia transaction dataset to predict housing price in our paper \"*Unfolding Beijing in a Hedonic Way*\". See this paper here: https://www.researchgate.net/publication/355732617_Unfolding_Beijing_in_a_Hedonic_Way. \n",
    "\n",
    "Real transaction records used to be accessible on the Lianjia websites. But sadly, due to some policy restrictions arised in 2021 just after we finished the paper, Lianjia no longer shows the transaction webpages to the public. As a consequence, the webpage example we provided in the paper, https://bj.lianjia.com/chengjiao/101084782030.html, is no longer available. \n",
    "\n",
    "Here, for illustration purpose, we show how to scrape the on-sale second-hand property data  (https://bj.lianjia.com/ershoufang/), instead of the sold-out transaction data (https://bj.lianjia.com/chengjiao/) from the Beijing Lianjia website. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import some packages, where: \n",
    "- `selenium` fetches the webpage source code； \n",
    "- `BeautifulSoup` organizes the fetched html code and searches the information we need in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A webpage, e.g. **Firefox** or **Chrome**, is need for `selenium` to open a webpage in the background. To call the browser, we need a corresponding **webdriver**. To download and know more about the webdrivers, see: \n",
    "- Firefox: https://github.com/mozilla/geckodriver/releases; \n",
    "- Chrome: https://chromedriver.chromium.org/. \n",
    "\n",
    "Here, we use the Firefox webdriver as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = webdriver.firefox.options.Options()\n",
    "options.headless = True # call the browser in the background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(options=options) # Firefox webdriver must be put into PATH (windows)\n",
    "driver.implicitly_wait(10) # set the maximum waiting time to load the webpage completely\n",
    "\n",
    "# If you want to use the Chrome webdriver, please uncomment and run the code in the following cell. \n",
    "\n",
    "# options = webdriver.chrome.options.Options()\n",
    "# options.headless = True\n",
    "# driver = webdriver.Chrome(options=options)\n",
    "# driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See an example objective webpage here: https://bj.lianjia.com/ershoufang/101114772718.html. What we want is a program that given a **url**, returns many information fields on the webpage, including attributes of the house, price, location, and etc. Here, we define a function `Scrape_Lianjia` to fulfill this demand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scrape_Lianjia(url, driver):\n",
    "    \n",
    "    # Fetch the page\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Load the page into BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    # Initialize the container as a dictionary\n",
    "    info = {}\n",
    "    \n",
    "    # Basic information\n",
    "    labels = soup.find_all('span', class_='label')\n",
    "    for label in labels:\n",
    "        content = label.next_sibling\n",
    "        while content.text.strip()=='':\n",
    "            content = content.next_sibling\n",
    "        info[label.text.strip()] = content.text.strip().replace('\\xa0', ' ').replace('举报', '').replace('㎡', '平米')\n",
    "    info.pop('风险提示')\n",
    "    \n",
    "    # Follower\n",
    "    follow = soup.find('span', class_='count')\n",
    "    info['关注人数'] = follow.text.strip()\n",
    "    \n",
    "    # Total Price\n",
    "    price_total = soup.find('span', class_='total')\n",
    "    price_unit = soup.find('span', class_='unit')\n",
    "    info['总价'] = price_total.text.strip()+price_unit.text.strip()+'元'\n",
    "    \n",
    "    # Unit Price\n",
    "    unitPrice = soup.find('span', class_='unitPriceValue')\n",
    "    info['单价'] = unitPrice.text.strip()\n",
    "    \n",
    "    # Construction Year\n",
    "    constructYear = soup.find('div', class_='subInfo noHidden')\n",
    "    info['建成时间'] = re.findall(r'.+年建', constructYear.text.strip())[0]\n",
    "    \n",
    "    # Coordinates and Uniqueness\n",
    "    scripts = soup.find_all('script')\n",
    "    for script in scripts:\n",
    "        isUnique = re.findall(r\"isUnique:'\\w+'\", script.text.strip())\n",
    "        coord = re.findall(r\"resblockPosition:'\\d+\\.\\d+,\\d+\\.\\d+'\", script.text.strip())\n",
    "        if isUnique != []:\n",
    "            isUnique = re.split(\"[':]\", isUnique[0])\n",
    "            info['是否唯一'] = isUnique[2]\n",
    "        if coord != []:\n",
    "            coord = re.split(\"[':,]\", coord[0])\n",
    "            info['Longitude'] = coord[2]\n",
    "            info['Latitude'] = coord[3]\n",
    "    \n",
    "    return info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how this program work on the example webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NavigableString' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8392/3287882271.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mScrape_Lianjia\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://bj.lianjia.com/ershoufang/101114772718.html'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8392/1186035074.py\u001b[0m in \u001b[0;36mScrape_Lianjia\u001b[1;34m(url, driver)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_sibling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[1;32mwhile\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_sibling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\xa0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'举报'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'㎡'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'平米'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    644\u001b[0m             raise AttributeError(\n\u001b[0;32m    645\u001b[0m                 \"'%s' object has no attribute '%s'\" % (\n\u001b[1;32m--> 646\u001b[1;33m                     self.__class__.__name__, attr))\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moutput_ready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"minimal\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NavigableString' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "Scrape_Lianjia('https://bj.lianjia.com/ershoufang/101114772718.html', driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program works well. Since one url is corresponding for one house on sale, then the next question is how to get all these urls. It can be easily observed that the lists of the on-sale houses are shown on a series of webpages from https://bj.lianjia.com/ershoufang/pg1/ to https://bj.lianjia.com/ershoufang/pg100/ in a good order, where `pg` stands for \"page\" and there are 100 pages of house lists under https://bj.lianjia.com/ershoufang/. Here, as an example, we collect all housing urls in the first 5 pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_max = 5 # set the number of pages to search here\n",
    "\n",
    "urls = []\n",
    "for p in list(range(1, page_max+1)):\n",
    "    driver.get('https://bj.lianjia.com/ershoufang/pg'+str(p)+'/')\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    items = soup.find_all('a', class_='noresultRecommend img LOGCLICKDATA')\n",
    "    for item in items:\n",
    "        urls.append(item['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what are contained in the list `urls`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, what we need to do is just run our program `Scrape_Lianjia` through all urls inside the list `urls`, and then save the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lianjia = pd.DataFrame()\n",
    "for url in urls:\n",
    "    info = Scrape_Lianjia(url, driver)\n",
    "    lianjia = lianjia.append(info, ignore_index=True)\n",
    "\n",
    "lianjia.to_csv('lianjia.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what we got! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lianjia.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
