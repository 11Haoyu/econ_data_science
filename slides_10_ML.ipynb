{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning in \n",
    "\n",
    "## Zhentao Shi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Machine learning and artificial intelligence:\n",
    "\n",
    "* Technology or alchemy?\n",
    "* Statistics or biology?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Friendman, Hastie and Tibshirani (2001, 2008): Elements of Statistical Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Athey (2018) \n",
    "* Mullainathan and Spiess (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "* Connection between $X$ and $Y$\n",
    "* Regression and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A set of data fitting procedures focusing on out-of-sample prediction\n",
    "* Repeat a scientific experiment for $n$ times and obtain a dataset $(y_i, x_i)_{i=1}^n$.\n",
    "* How to best predict $y_{n+1}$ given $x_{n+1}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "* Only about $X$\n",
    "* Density estimation, principal component analysis, and clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Conventional Statistics\n",
    "\n",
    "* Consistency\n",
    "* Asymptotic distribution (hopefully normal)\n",
    "* Efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning's Responses\n",
    "\n",
    "* Efficiency is mostly irrelevant given big data\n",
    "* Statistical inference may not be the goal\n",
    "    * Recommendation system on Amazon or Taobao\n",
    "    * Care about the prediction accuracy, not the causal link\n",
    "* Is there a data generating process (DGP)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nonparametric Estimation\n",
    "\n",
    "* *Parametric*: a finite number of parameters\n",
    "* *Nonparametric*: an infinite number of parameters\n",
    "\n",
    "* Some ideas in nonparametric estimation is directly related to machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Density Estimation\n",
    "\n",
    "* Density estimation given a sample $(x_1,\\ldots,x_n)$\n",
    "* If drawn from a parametric family, MLE for estimation\n",
    "* Misspecification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Histogram is nonparametric\n",
    "    * If grid too fine, small bias but large variance\n",
    "    * If grid too coarse, small variance but large bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Variance-Bias Tradeoff\n",
    "\n",
    "![](bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Conditional Mean\n",
    "\n",
    "* Conditional mean $$f(x) = E[y_i |x_i = x]$$ given a sample $(y_i, x_i)$. \n",
    "* Solve \n",
    "$$\n",
    "\\min_f E[ (y_i - f(x_i) )^2 ]\n",
    "$$\n",
    "* In general $f(x)$ is a nonlinear function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Restrict the class of functions to search for minimizer\n",
    "    * Assume differentiability\n",
    "* One way is kernel method based on density estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Series Estimation\n",
    "\n",
    "* Series expansion to approximate $f(x)$\n",
    "* Generates many additive regressors\n",
    "    * Ex: bounded, continuous and differentiate function has a series\n",
    "representation $f(x) = \\sum_{k=0}^{\\infty} \\beta_k \\cos (\\frac{k}{2}\\pi x )$.\n",
    "    * In finite sample, choose a finite $K$, usually much smaller than $n$\n",
    "    * Asymptotically $K \\to \\infty$ as $n \\to \\infty$ so that\n",
    "$$\n",
    "f_K(x) = \\sum_{k=0}^{K} \\beta_k \\cos \\left(\\frac{k}{2}\\pi x \\right) \\to f(x).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Bias-variance trade-off\n",
    "    * Big $K$: small bias and large variance \n",
    "    * Small $K$: small variance and large bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Penalization\n",
    "\n",
    "* Specify a sufficiently large $K$, and then add a penalty term to control the complexity\n",
    "* Eg: *Ridge regression*: \n",
    "$$\n",
    "\\min_\\beta \\  \\frac{1}{2n}  \\sum_{i=1}^n \\left(y_i - \\sum_{k=0}^{K} \\beta_k f_k(x_i) \\right)^2\n",
    "+ \\lambda \\sum_{k=0}^K \\beta_k^2,\n",
    "$$\n",
    "where $\\lambda$ is the tuning parameter such that $\\lambda \\to 0$ as $n\\to \\infty$, and\n",
    "$f_k(x_i) = \\cos \\left(\\frac{k}{2}\\pi x_i \\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In compact notation, let $Y=(y_1,\\ldots,y_n)'$ and\n",
    "$X = (X_{ik} = f_k(x_i) )$, the above problem can be written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_2 ^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tuning Parameter\n",
    "\n",
    "* *Information criterion*: AIC, BIC\n",
    "* *Cross validation*\n",
    "\n",
    "\n",
    "* Active statistical research, but has little economics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Econometrics Workflow\n",
    "\n",
    "![](metric_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting\n",
    "\n",
    "![ ](graph/ML_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `Caret` Package\n",
    "\n",
    "* R package `caret` (Classification And REgression Training): a framework for many machine learning methods\n",
    "* The function [`createDataPartition`](https://topepo.github.io/caret/data-splitting.html)\n",
    "splits the sample for both cross sectional data and time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Selection\n",
    "\n",
    "* Number of covariates $x_i$ can be enormous.\n",
    "    * DNA microarray\n",
    "    * UK Living Costs and Food Survey\n",
    "    * Giannone, Lenza, and Primiceri (2017): Illusion of sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Conventional attitude: prior knowledge\n",
    "* Recently economists wake up from the long lasting negligence.\n",
    "    * Stock and Watson (2012): forecasting 143 US macroeconomic indicators.\n",
    "    * A horse race of several variable selection methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lasso\n",
    "\n",
    "* least-absolute-shrinkage-and-selection-operator\n",
    "(Lasso) (Tibshirani, 1996)\n",
    "* Penalizes the $L_1$ norm of the coefficients.\n",
    "The criterion function of Lasso is written as\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\Vert \\beta \\Vert_1\n",
    "$$\n",
    "where $\\lambda \\geq 0$ is a tuning parameter. \n",
    "\n",
    "Lasso shrinks some coefficients exactly to 0, in a wide range of values of $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "![ ](graph/lasso_regression2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SCAD\n",
    "\n",
    "* Smoothly-clipped-absolute-deviation (SCAD) Fan and Li (2001):\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\sum_{j=1}^d \\rho_{\\lambda}( |\\beta_j| )\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\rho_{\\lambda}^{\\prime} (\\theta) = \\lambda \\left\\{ 1\\{\\theta\\leq \\lambda \\} +\n",
    "\\frac{(a\\lambda - \\theta)_+}{(a-1)\\lambda} \\cdot 1 \\{\\theta > \\lambda\\} \\right\\}\n",
    "$$\n",
    "for some $a>2$ and $\\theta>0$. \n",
    "\n",
    "* SCAD enjoys *oracle property*. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](SCAD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adaptive Lasso\n",
    "\n",
    "*Adaptive Lasso* (Zou, 2006) also enjoys the oracle property.\n",
    "\n",
    "Two-step algorithm:\n",
    "1. First run a Lasso or ridge regression and save the estimator $\\hat{\\beta}^{(1)}$\n",
    "2. Solve \n",
    "\n",
    "$$\n",
    "(2n)^{-1} (Y-X\\beta)'(Y-X\\beta) + \\lambda \\sum_{j=1}^d  w_j |\\beta_j|\n",
    "$$ \n",
    "\n",
    "where $w_j = 1 /  |\\hat{\\beta}_j^{(1)} |^a$ and $a\\geq 1$ is a constant. (Common choice is $a = 1$ or 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "R packages\n",
    "\n",
    "* `glmnet` or `LARS` implements Lasso\n",
    "* `ncvreg` carries out SCAD. \n",
    "* Adaptive Lasso by setting the weight via the argument `penalty.factor` in `glmnet`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stagewise Forward Selection\n",
    "\n",
    "More methods are available if prediction of the response variables is the sole purpose of the regression.\n",
    "\n",
    "Eg: *stagewise forward selection*\n",
    "\n",
    "1. Start from an empty model. \n",
    "2. Given many candidate $x_j$, in each round we add the regressor that can\n",
    "produce the biggest $R^2$. \n",
    "\n",
    "Close to the idea of *$L_2$ componentwise boosting*\n",
    "which does not adjust the coefficients fitted earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Second Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Prediction-Oriented Methods\n",
    "\n",
    "* Methods that induces data-driven interaction of the covariates.\n",
    "* Interaction makes the covariates much more flexible\n",
    "* Insufficient theoretical understanding\n",
    "* \"Black-boxes\" methods\n",
    "\n",
    "* Surprisingly superior performance\n",
    "* Industry insiders are pondering \"alchemy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression Tree\n",
    "\n",
    "* Supervised learning: $x \\to y $\n",
    "* Traditional nonparametric methods: kernel or series\n",
    "\n",
    "* Regression tree (Breiman, 1984) recursively partitions the space of the regressors\n",
    "    * Each time a covariate is split into two dummies\n",
    "    * Splitting criterion is aggressive reduction of the SSR\n",
    "    \n",
    "    * Tuning parameter is the depth of the tree\n",
    "    * Given a dataset $d$ and the depth of the tree, the fitted tree $\\hat{r}(d)$ is deterministic\n",
    "\n",
    "\n",
    "- Example: Using longitude and latitude for Beijing housing price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bagging\n",
    "\n",
    "* Tree is unstable\n",
    "* *Bootstrap averaging*, or *bagging*, reduces variance of trees (Breiman, 1996)\n",
    "    * Grow a tree for each bootstrap sample\n",
    "    * Simple average\n",
    "\n",
    "* An example of the *ensemble learning*.\n",
    "\n",
    "* Inoue and Kilian (2008): an early application of bagging in time series forecast.\n",
    "* Hirano and Wright (2017): a theoretical perspective on the risk reduction of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "* *Random forest* (Breiman, 2001):\n",
    "    * Draw a bootstrap sample\n",
    "    * Before each split, shakes up the regressors by randomly sampling $m$ out of the total $p$ covarites. Stop until the depth of the tree is reached.\n",
    "    * Average the trees over the bootstrap samples\n",
    "    \n",
    "* The tuning parameters are the tree depth and $m$\n",
    "* More stable than bagging thanks to \"de-correlation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: randomForest\n",
      "randomForest 4.6-14\n",
      "Type rfNews() to see new features/changes/bug fixes.\n",
      "Loading required package: MASS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>IncNodePurity</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>crim</th><td>1476.8398</td></tr>\n",
       "\t<tr><th scope=row>zn</th><td> 138.0405</td></tr>\n",
       "\t<tr><th scope=row>indus</th><td> 930.0199</td></tr>\n",
       "\t<tr><th scope=row>chas</th><td> 265.7788</td></tr>\n",
       "\t<tr><th scope=row>nox</th><td>1831.5730</td></tr>\n",
       "\t<tr><th scope=row>rm</th><td>7551.2292</td></tr>\n",
       "\t<tr><th scope=row>age</th><td> 567.3393</td></tr>\n",
       "\t<tr><th scope=row>dis</th><td>1296.5395</td></tr>\n",
       "\t<tr><th scope=row>rad</th><td> 280.9853</td></tr>\n",
       "\t<tr><th scope=row>tax</th><td> 639.1153</td></tr>\n",
       "\t<tr><th scope=row>ptratio</th><td>1990.4504</td></tr>\n",
       "\t<tr><th scope=row>black</th><td> 469.8464</td></tr>\n",
       "\t<tr><th scope=row>lstat</th><td>7076.4031</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|l}\n",
       "  & IncNodePurity\\\\\n",
       "\\hline\n",
       "\tcrim & 1476.8398\\\\\n",
       "\tzn &  138.0405\\\\\n",
       "\tindus &  930.0199\\\\\n",
       "\tchas &  265.7788\\\\\n",
       "\tnox & 1831.5730\\\\\n",
       "\trm & 7551.2292\\\\\n",
       "\tage &  567.3393\\\\\n",
       "\tdis & 1296.5395\\\\\n",
       "\trad &  280.9853\\\\\n",
       "\ttax &  639.1153\\\\\n",
       "\tptratio & 1990.4504\\\\\n",
       "\tblack &  469.8464\\\\\n",
       "\tlstat & 7076.4031\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | IncNodePurity |\n",
       "|---|---|\n",
       "| crim | 1476.8398 |\n",
       "| zn |  138.0405 |\n",
       "| indus |  930.0199 |\n",
       "| chas |  265.7788 |\n",
       "| nox | 1831.5730 |\n",
       "| rm | 7551.2292 |\n",
       "| age |  567.3393 |\n",
       "| dis | 1296.5395 |\n",
       "| rad |  280.9853 |\n",
       "| tax |  639.1153 |\n",
       "| ptratio | 1990.4504 |\n",
       "| black |  469.8464 |\n",
       "| lstat | 7076.4031 |\n",
       "\n"
      ],
      "text/plain": [
       "        IncNodePurity\n",
       "crim    1476.8398    \n",
       "zn       138.0405    \n",
       "indus    930.0199    \n",
       "chas     265.7788    \n",
       "nox     1831.5730    \n",
       "rm      7551.2292    \n",
       "age      567.3393    \n",
       "dis     1296.5395    \n",
       "rad      280.9853    \n",
       "tax      639.1153    \n",
       "ptratio 1990.4504    \n",
       "black    469.8464    \n",
       "lstat   7076.4031    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Plot with title \"Boston.rf\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "require(randomForest)\n",
    "require(MASS)#Package which contains the Boston housing dataset\n",
    "attach(Boston)\n",
    "set.seed(101)\n",
    "\n",
    "#training Sample with 300 observations\n",
    "train=sample(1:nrow(Boston),300)\n",
    "\n",
    "Boston.rf=randomForest(medv ~ . , data = Boston, subset = train)\n",
    "plot(Boston.rf)\n",
    "\n",
    "# getTree(Boston.rf)\n",
    "importance(Boston.rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Consistency of random forest is not proved\n",
    "until Scornet, Biau, and Vert (2015)\n",
    "* Inferential theory was first established by\n",
    "Wager Athey (2018)  in the context of treatment effect estimation\n",
    "* Athey, Tibshirani, and Wager (2019) generalizes CART to local maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "* Bagging and random forest use equal weight on each generated tree for the ensemble\n",
    "* Tree boosting takes a deterministic approach for the weights\n",
    "    1. Use the original data $d^0=(x_i,y_i)$ to grow a shallow tree $\\hat{r}^{0}(d^0)$. Save the prediction $f^0_i = \\alpha \\cdot \\hat{r}^0 (d^0, x_i)$ where\n",
    "   $\\alpha\\in [0,1]$ is a shrinkage tuning parameter. Save\n",
    "   the residual $e_i^{0} = y_i - f^0_i$. Set $m=1$.\n",
    "    2. In the $m$-th iteration, use the data $d^m = (x_i,e_i^{m-1})$ to grow a shallow tree $\\hat{r}^{m}(d^m)$. Save the prediction $f^m_i =  f^{m-1}_i +  \\alpha \\cdot \\hat{r}^m (d, x_i)$. Save\n",
    "   the residual $e_i^{m} = y_i - f^m_i$. Update $m = m+1$.\n",
    "    3. Repeat Step 2 until $m > M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Boosting has three tuning parameters: the tree depth,  the shrinkage level $\\alpha$, and the number of iterations $M$\n",
    "* The algorithm can be sensitive to any of the three tuning parameters\n",
    "* When a model is tuned well, it can performs remarkably\n",
    "    * Example: Beijing housing data.\n",
    "    * Gradient boosting via the package `gbm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Statisticians view boosting as a gradient descent algorithm to reduce the risk. The fitted\n",
    "tree in each iteration is the deepest descent direction, while the shrinkage tames the fitting to avoid proceeding too aggressively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n",
      "\n",
      "Attaching package: 'ggplot2'\n",
      "\n",
      "The following object is masked from 'package:randomForest':\n",
      "\n",
      "    margin\n",
      "\n",
      "Loading required package: foreach\n",
      "Loading required package: iterators\n",
      "Loading required package: parallel\n",
      "Warning message in readChar(con, 5L, useBytes = TRUE):\n",
      "\"cannot open compressed file 'lianjia.RData', probable reason 'No such file or directory'\""
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection\n",
     "output_type": "error",
     "traceback": [
      "Error in readChar(con, 5L, useBytes = TRUE): cannot open the connection\nTraceback:\n",
      "1. load(\"lianjia.RData\")",
      "2. readChar(con, 5L, useBytes = TRUE)"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "library(doParallel)\n",
    "\n",
    "load(\"lianjia.RData\")\n",
    "\n",
    "price_reg=price~\n",
    "  t_trade+age+fiveYearsProperty+subway+factor(district)+dist_center+\n",
    "  square+livingRoom+drawingRoom+kitchen+bathRoom+\n",
    "  factor(floor_type)+floor_total+elevator+ladderRatio+\n",
    "  factor(renovationCondition)+factor(buildingType)+factor(buildingStructure)+\n",
    "  communityAverage+DOM+followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# GBM\n",
    "## Tuning Parameters\n",
    "\n",
    "gbmGrid=expand.grid(interaction.depth=10:14,\n",
    "                    n.trees=(15:20)*100,\n",
    "                    shrinkage=c(0.01,0.05,0.1),\n",
    "                    n.minobsinnode=20)\n",
    "gbmControl=trainControl(method=\"repeatedcv\",number=5,repeats=1)\n",
    "\n",
    "registerDoParallel(8)\n",
    "t=Sys.time()\n",
    "boostingReg=train(price_reg,data=lianjia[train_tune,],\n",
    "                  method=\"gbm\",distribution=\"gaussian\",\n",
    "                  trControl=gbmControl,tuneGrid=gbmGrid,metric=\"Rsquared\",\n",
    "                  verbose=F)\n",
    "cat(\"Time Cost of Finding Best Tuning Parameters:\",Sys.time()-t,\"\\n\")\n",
    "stopImplicitCluster()\n",
    "\n",
    "gbmTune=boostingReg$bestTune\n",
    "cat(\"The best tuning parameters for GBM are: \\n\");print(gbmTune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Estimation and Prediction\n",
    "\n",
    "train_ind=createDataPartition(1:nrow(lianjia),p=0.75)$Resample1\n",
    "boostingReg=train(price_reg,data=lianjia[train_ind,],method=\"gbm\",\n",
    "                  distribution=\"gaussian\", # to decide the lost function\n",
    "                  tuneGrid=gbmTune,verbose=F)\n",
    "pred.boosting=predict(boostingReg,newdata=lianjia[-train_ind,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# LM\n",
    "\n",
    "lmReg=lm(price_reg,data=lianjia[train_ind,])\n",
    "pred.lm=predict(lmReg,newdata=lianjia[-train_ind,])\n",
    "\n",
    "\n",
    "# Comparison\n",
    "\n",
    "target=lianjia[-train_ind,]$price\n",
    "cat(\"R-squared of GBM prediction =\",miscTools::rSquared(target,target-pred.boosting),\"\\n\")\n",
    "cat(\"R-squared of LM prediction =\",miscTools::rSquared(target,target-pred.lm),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Many variants of boosting algorithms\n",
    "    * $L_2$-boosting\n",
    "    * componentwise boosting\n",
    "    * AdaBoosting, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "* Artificial neural network (ANN) is the workhorse behind Alpha-Go and self-driven cars\n",
    "* A particular type of nonlinear models.\n",
    "\n",
    "![ANN](Colored_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The transition from layer $k-1$ to layer $k$ can be written as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "z_l^{(k)} & = & w_{l0}^{(k-1)} + \\sum_{j=1}^{p_{k-1} } w_{lj}^{(k-1)} a_j^{(k-1)} \\\\ \n",
    "a_l^{(k)} & = & g^{(k)} ( z_l^{(k)}), \n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "where $ a_j^{(0)} = x_j$ is the input.\n",
    "\n",
    "* The latent variable $z_l^{(k)}$ usually takes a linear form\n",
    "* *Activation function* $g(\\cdot)$ is usually a simple nonlinear function\n",
    "* Popular choice: sigmoid ($1/(1+\\exp(-x))$); ReLu, $z\\cdot 1\\{x\\geq 0\\}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A user has several decisions to make\n",
    "* Activation function\n",
    "* Number of hidden layers\n",
    "* Number of nodes in each layer\n",
    "\n",
    "\n",
    "* Many free parameters are generated from the multiple layer and multiple nodes\n",
    "* In estimation often regularization methods are employed to penalize\n",
    "the $l_1$ and/or $l_2$ norms, which requires extra tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Theory is Underdeveloped\n",
    "\n",
    "* Theoretical understanding about its behavior is scant\n",
    "* Hornik, Stinchcombe, and White (1989):\n",
    "    * A single hidden layer neural network, given enough many nodes, is a *universal approximator* for any\n",
    "measurable function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Computation\n",
    "\n",
    "* Free parameters must be determined by\n",
    "numerical optimization\n",
    "* Nonlinear complex structure makes the optimization\n",
    "very challenging and the global optimizer is beyond guarantee\n",
    "* De facto optimization algorithm\n",
    "is *stochastic gradient descent*\n",
    "\n",
    "* Google's `tensorflow`\n",
    "* `keras` is the deep learning modeling language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "In optimization we update the parameter\n",
    "$$\n",
    "\\beta_{k+1} = \\beta_{k} + a_k p_k,\n",
    "$$\n",
    "where $a_k \\in \\mathbb{R}$ is the step length and $p_k$ is a vector\n",
    "of directions. \n",
    "\n",
    "Use a Talyor expansion,\n",
    "$$\n",
    "f(\\beta_{k+1}) = f(\\beta_k + a_k p_k ) \\approx f(\\beta_k) + a_k \\nabla f(\\beta_k) p_k,\n",
    "$$\n",
    "\n",
    "\n",
    "If in each step we want the value of the criterion function\n",
    "$f(x)$ to decrease, we need $\\nabla f(\\beta_k) p_k \\leq 0$.\n",
    "A simple choice is $p_k =-\\nabla f(\\beta_k)$.\n",
    "\n",
    "Newton's method corresponds to $p_k =- (\\nabla^2 f(\\beta_k))^{-1}  \\nabla f(\\beta_k)$,\n",
    "and BFGS uses a low-rank matrix to approximate $\\nabla^2 f(\\beta_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* When sample size and/or number of parameter is big, evaluation of the gradient can be prohibitively expensive.\n",
    "* SGD uses a small batch of the sample to evaluate the gradient in each iteration. \n",
    "\n",
    "* SGD involves tuning parameters (say, the batch size and the learning rate) that can dramatically affect\n",
    "the outcome, in particular in nonlinear problems.\n",
    "* Careful experiments must be carried out before serious implementation.\n",
    "\n",
    "Below is an example of SGD in the PPMLE with sample size 100,000 and\n",
    "the number of parameters 100. SGD is usually much faster.\n",
    "\n",
    "The new functions are defined with the data explicity as arguments.\n",
    "Because in SGD each time the log-likelihood function and the gradiant are\n",
    "evaluated at a different subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "poisson.loglik = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  exp( X %*% b )\n",
    "  ell = -mean( -lambda + y *  log(lambda) )\n",
    "  return(ell)\n",
    "}\n",
    "\n",
    "\n",
    "poisson.loglik.grad = function( b, y, X ) {\n",
    "  b = as.matrix( b )\n",
    "  lambda =  as.vector( exp( X %*% b ) )\n",
    "  ell = -colMeans( -lambda * X + y * X )\n",
    "  ell_eta = ell\n",
    "  return(ell_eta)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "##### generate the artificial data\n",
    "set.seed(898)\n",
    "nn = 1e5; K = 100\n",
    "\n",
    "X = cbind(1, matrix( runif( nn*(K-1) ), ncol = K-1 ) )\n",
    "b0 = rep(1, K) / K\n",
    "y = rpois(nn, exp( X %*% b0 ) )\n",
    "\n",
    "\n",
    "b.init = runif(K); b.init  = 2 * b.init / sum(b.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# and these tuning parameters are related to N and K\n",
    "\n",
    "n = length(y)\n",
    "test_ind = sample(1:n, round(0.2*n) )\n",
    "\n",
    "y_test = y[test_ind]\n",
    "X_test = X[test_ind, ]\n",
    "\n",
    "y_train = y[-test_ind ]\n",
    "X_train = X[-test_ind, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "\n",
    "# sgd depends on\n",
    "# * eta: the learning rate\n",
    "# * epoch: the averaging small batch\n",
    "# * the initial value\n",
    "\n",
    "set.seed(105)\n",
    "\n",
    "max_iter = 5000\n",
    "min_iter = 20\n",
    "eta=0.01\n",
    "epoch = round( 100*sqrt(K) )\n",
    "\n",
    "\n",
    "b_old = b.init\n",
    "\n",
    "pts0 = Sys.time()\n",
    "# the iteration of gradient\n",
    "for (i in 1:max_iter ){\n",
    "\n",
    "  loglik_old = poisson.loglik(b_old, y_train, X_train)\n",
    "  i_sample = sample(1:length(y_train), epoch, replace = TRUE )\n",
    "  b_new = b_old - eta * poisson.loglik.grad(b_old, y_train[i_sample], X_train[i_sample, ])\n",
    "  loglik_new = poisson.loglik(b_new, y_test, X_test)\n",
    "  b_old = b_new # update\n",
    "\n",
    "  criterion =  loglik_old - loglik_new  \n",
    "  if (  criterion < 0.0001 & i >= min_iter ) break\n",
    "}\n",
    "cat(\"point estimate =\", b_new, \", log_lik = \", loglik_new, \"\\n\")\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# optimx is too slow for this dataset.\n",
    "# Nelder-Mead method is too slow for this dataset\n",
    "\n",
    "# thus we only sgd with NLoptr\n",
    "\n",
    "opts = list(\"algorithm\"=\"NLOPT_LD_SLSQP\",\"xtol_rel\"=1.0e-7, maxeval = 5000)\n",
    "\n",
    "\n",
    "pts0 = Sys.time( )\n",
    "res_BFGS = nloptr::nloptr( x0=b.init,\n",
    "                 eval_f=poisson.loglik,\n",
    "                 eval_grad_f = poisson.loglik.grad,\n",
    "                 opts=opts,\n",
    "                 y = y_train, X = X_train)\n",
    "print( res_BFGS )\n",
    "pts1 = Sys.time( ) - pts0\n",
    "print(pts1)\n",
    "\n",
    "b_hat_nlopt = res_BFGS$solution\n",
    "\n",
    "\n",
    "#### evaluation in the test sample\n",
    "cat(\"\\n\\n\\n\\n\\n\\n\\n\")\n",
    "cat(\"log lik in test data by sgd = \", poisson.loglik(b_new, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by nlopt = \", poisson.loglik(b_hat_nlopt, y = y_test, X_test), \"\\n\")\n",
    "cat(\"log lik in test data by oracle = \", poisson.loglik(b0, y = y_test, X_test), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "* Mature algorithms for implementation\n",
    "* Theoretical investigation is in progress\n",
    "* Economic applications are just beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Lehrer and Xie (2017) \n",
    "* Feng, Giglio, and Xiu (2019)\n",
    "* Chinco, Clark-Joseph, and Ye (2019)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernel_info": {
   "name": "ir"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.0"
  },
  "nteract": {
   "version": "0.15.0"
  },
  "rise": {
   "scroll": "true",
   "theme": "serif"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
